{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 & 3: Model Development and Confidence Scoring\n",
    "## Career Recommendation Engine\n",
    "\n",
    "This notebook covers:\n",
    "- Multi-label classification model training (Random Forest & XGBoost)\n",
    "- Hyperparameter optimization\n",
    "- Model evaluation with comprehensive metrics\n",
    "- Error analysis\n",
    "- Confidence score engineering and validation\n",
    "- Model persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, label_ranking_average_precision_score\n",
    "\n",
    "from model_trainer import CareerModelTrainer\n",
    "from confidence_scorer import ConfidenceScorer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from Task 1\n",
    "X = pd.read_csv('../data/processed/features.csv')\n",
    "y = np.load('../data/processed/targets.npy')\n",
    "metadata = joblib.load('../data/processed/metadata.pkl')\n",
    "\n",
    "feature_names = metadata['feature_names']\n",
    "career_names = metadata['career_names']\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"Number of careers: {len(career_names)}\")\n",
    "print(f\"\\nCareer names: {career_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split training data for validation (for calibration)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Training subset: {X_train_sub.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = CareerModelTrainer(random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model, rf_train_metrics = trainer.train_random_forest(\n",
    "    X_train.values, y_train, optimize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_model, xgb_train_metrics = trainer.train_xgboost(\n",
    "    X_train.values, y_train, optimize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on test set\n",
    "comparison_df = trainer.compare_models(X_test.values, y_test)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics_to_plot = ['hamming_loss', 'label_ranking_avg_precision', 'precision_at_3', 'subset_accuracy']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    data = comparison_df[['model', metric]]\n",
    "    axes[i].bar(data['model'], data[metric], color=['#3498db', '#e74c3c'], alpha=0.7)\n",
    "    axes[i].set_title(metric.replace('_', ' ').title(), fontweight='bold')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(data[metric]):\n",
    "        axes[i].text(j, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {trainer.best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform error analysis\n",
    "error_df = trainer.analyze_errors(\n",
    "    X_test.values, y_test, career_names, n_samples=20\n",
    ")\n",
    "\n",
    "print(\"\\nSample Misclassifications:\")\n",
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix for Top Careers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Get predictions\n",
    "y_pred = trainer.best_model.predict(X_test.values)\n",
    "\n",
    "# Calculate confusion matrix for each career\n",
    "cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrices for top 6 careers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get top 6 most common careers\n",
    "career_counts = y_test.sum(axis=0)\n",
    "top_career_indices = np.argsort(career_counts)[-6:][::-1]\n",
    "\n",
    "for i, career_idx in enumerate(top_career_indices):\n",
    "    sns.heatmap(cm[career_idx], annot=True, fmt='d', cmap='Blues', \n",
    "                ax=axes[i], cbar=False, square=True)\n",
    "    axes[i].set_title(f'{career_names[career_idx]}', fontweight='bold')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    axes[i].set_xticklabels(['Negative', 'Positive'])\n",
    "    axes[i].set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Probability Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "y_pred_proba = trainer.best_model.predict_proba(X_test.values)\n",
    "\n",
    "# Convert to probability matrix\n",
    "y_score = np.zeros_like(y_test, dtype=float)\n",
    "for i, proba_array in enumerate(y_pred_proba):\n",
    "    y_score[:, i] = proba_array[:, 1]\n",
    "\n",
    "# Plot probability distribution for positive vs negative samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Positive samples\n",
    "positive_probs = y_score[y_test == 1]\n",
    "axes[0].hist(positive_probs, bins=50, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Probability Distribution for True Positive Labels', fontweight='bold')\n",
    "axes[0].axvline(positive_probs.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {positive_probs.mean():.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Negative samples\n",
    "negative_probs = y_score[y_test == 0]\n",
    "axes[1].hist(negative_probs, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Probability Distribution for True Negative Labels', fontweight='bold')\n",
    "axes[1].axvline(negative_probs.mean(), color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {negative_probs.mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nProbability Calibration Analysis:\")\n",
    "print(f\"Mean probability for positive labels: {positive_probs.mean():.4f}\")\n",
    "print(f\"Mean probability for negative labels: {negative_probs.mean():.4f}\")\n",
    "print(f\"Separation: {positive_probs.mean() - negative_probs.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confidence Score Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize confidence scorer\n",
    "confidence_scorer = ConfidenceScorer(career_names)\n",
    "\n",
    "print(\"‚úì Confidence Scorer initialized\")\n",
    "print(f\"Career requirements defined for {len(confidence_scorer.career_skill_requirements)} careers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Confidence Scoring on Sample Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: Technical Profile\n",
    "test_case_1 = {\n",
    "    'skills': ['Python', 'Machine Learning', 'Statistics', 'Data Analysis'],\n",
    "    'technical_skills_count': 4,\n",
    "    'soft_skills_count': 0,\n",
    "    'total_skills': 4,\n",
    "    'analytical': 0.9,\n",
    "    'creative': 0.3,\n",
    "    'social': 0.4,\n",
    "    'education_encoded': 3,\n",
    "    'education': 'Master',\n",
    "    'experience': 5,\n",
    "    'tech_oriented': 3,\n",
    "    'creative_oriented': 0,\n",
    "    'business_oriented': 0,\n",
    "    'social_oriented': 0\n",
    "}\n",
    "\n",
    "# Get model predictions for test case\n",
    "X_test_case_1 = np.array([[test_case_1[f] if f in test_case_1 else 0 for f in feature_names]])\n",
    "y_pred_proba_1 = trainer.best_model.predict_proba(X_test_case_1)\n",
    "probabilities_1 = np.array([proba[0][1] for proba in y_pred_proba_1])\n",
    "\n",
    "# Calculate confidence scores\n",
    "recommendations_1 = confidence_scorer.calculate_confidence_scores(\n",
    "    probabilities_1, test_case_1, top_k=5\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST CASE 1: TECHNICAL PROFILE\")\n",
    "print(\"=\"*70)\n",
    "print(confidence_scorer.generate_confidence_report(recommendations_1, test_case_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Creative Profile\n",
    "test_case_2 = {\n",
    "    'skills': ['UI/UX', 'Creative Writing', 'Communication'],\n",
    "    'technical_skills_count': 1,\n",
    "    'soft_skills_count': 2,\n",
    "    'total_skills': 3,\n",
    "    'analytical': 0.3,\n",
    "    'creative': 0.9,\n",
    "    'social': 0.7,\n",
    "    'education_encoded': 2,\n",
    "    'education': 'Bachelor',\n",
    "    'experience': 2,\n",
    "    'tech_oriented': 0,\n",
    "    'creative_oriented': 2,\n",
    "    'business_oriented': 0,\n",
    "    'social_oriented': 1\n",
    "}\n",
    "\n",
    "X_test_case_2 = np.array([[test_case_2[f] if f in test_case_2 else 0 for f in feature_names]])\n",
    "y_pred_proba_2 = trainer.best_model.predict_proba(X_test_case_2)\n",
    "probabilities_2 = np.array([proba[0][1] for proba in y_pred_proba_2])\n",
    "\n",
    "recommendations_2 = confidence_scorer.calculate_confidence_scores(\n",
    "    probabilities_2, test_case_2, top_k=5\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST CASE 2: CREATIVE PROFILE\")\n",
    "print(\"=\"*70)\n",
    "print(confidence_scorer.generate_confidence_report(recommendations_2, test_case_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3: Business Profile\n",
    "test_case_3 = {\n",
    "    'skills': ['Business Strategy', 'Project Management', 'Leadership', 'Excel'],\n",
    "    'technical_skills_count': 1,\n",
    "    'soft_skills_count': 3,\n",
    "    'total_skills': 4,\n",
    "    'analytical': 0.6,\n",
    "    'creative': 0.5,\n",
    "    'social': 0.8,\n",
    "    'education_encoded': 3,\n",
    "    'education': 'Master',\n",
    "    'experience': 7,\n",
    "    'tech_oriented': 0,\n",
    "    'creative_oriented': 0,\n",
    "    'business_oriented': 3,\n",
    "    'social_oriented': 0\n",
    "}\n",
    "\n",
    "X_test_case_3 = np.array([[test_case_3[f] if f in test_case_3 else 0 for f in feature_names]])\n",
    "y_pred_proba_3 = trainer.best_model.predict_proba(X_test_case_3)\n",
    "probabilities_3 = np.array([proba[0][1] for proba in y_pred_proba_3])\n",
    "\n",
    "recommendations_3 = confidence_scorer.calculate_confidence_scores(\n",
    "    probabilities_3, test_case_3, top_k=5\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST CASE 3: BUSINESS PROFILE\")\n",
    "print(\"=\"*70)\n",
    "print(confidence_scorer.generate_confidence_report(recommendations_3, test_case_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validate Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate confidence scores on test set\n",
    "validation_results = []\n",
    "\n",
    "# Sample 100 test cases for validation\n",
    "sample_size = min(100, len(X_test))\n",
    "sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    # Get actual careers for this sample\n",
    "    actual_careers = [career_names[i] for i in range(len(career_names)) if y_test[idx, i] == 1]\n",
    "    \n",
    "    # Get model predictions\n",
    "    X_sample = X_test.iloc[idx:idx+1].values\n",
    "    y_pred_proba = trainer.best_model.predict_proba(X_sample)\n",
    "    probabilities = np.array([proba[0][1] for proba in y_pred_proba])\n",
    "    \n",
    "    # Create feature dict (simplified)\n",
    "    user_features = {\n",
    "        'skills': [],\n",
    "        'technical_skills_count': X_test.iloc[idx]['technical_skills_count'],\n",
    "        'analytical': X_test.iloc[idx]['analytical'],\n",
    "        'creative': X_test.iloc[idx]['creative'],\n",
    "        'social': X_test.iloc[idx]['social'],\n",
    "        'education_encoded': X_test.iloc[idx]['education_encoded'],\n",
    "        'experience': X_test.iloc[idx]['experience'],\n",
    "        'tech_oriented': X_test.iloc[idx]['tech_oriented']\n",
    "    }\n",
    "    \n",
    "    # Get confidence scores\n",
    "    recommendations = confidence_scorer.calculate_confidence_scores(\n",
    "        probabilities, user_features, top_k=5\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    validation_metrics = confidence_scorer.validate_confidence_scores(\n",
    "        recommendations, actual_careers\n",
    "    )\n",
    "    \n",
    "    validation_results.append(validation_metrics)\n",
    "\n",
    "# Aggregate validation results\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIDENCE SCORE VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nMean Validation Metrics:\")\n",
    "print(validation_df.mean())\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(validation_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['top_1_accuracy', 'top_3_accuracy', 'avg_confidence_correct', 'confidence_separation']\n",
    "titles = ['Top-1 Accuracy', 'Top-3 Accuracy', 'Avg Confidence (Correct)', 'Confidence Separation']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "for i, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n",
    "    axes[i].hist(validation_df[metric], bins=30, color=color, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_xlabel('Score')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(title, fontweight='bold')\n",
    "    axes[i].axvline(validation_df[metric].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {validation_df[metric].mean():.3f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "trainer.save_model('../models/career_recommender_v1.pkl')\n",
    "\n",
    "# Save additional artifacts\n",
    "artifacts = {\n",
    "    'feature_names': feature_names,\n",
    "    'career_names': career_names,\n",
    "    'test_metrics': comparison_df.to_dict(),\n",
    "    'validation_metrics': validation_df.mean().to_dict(),\n",
    "    'model_version': '1.0'\n",
    "}\n",
    "\n",
    "joblib.dump(artifacts, '../models/model_artifacts_v1.pkl')\n",
    "\n",
    "print(\"‚úì Model and artifacts saved successfully!\")\n",
    "print(f\"  - Model: ../models/career_recommender_v1.pkl\")\n",
    "print(f\"  - Artifacts: ../models/model_artifacts_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Performance:\n",
    "- **Best Model**: {best_model_name}\n",
    "- **Hamming Loss**: {hamming_loss:.4f}\n",
    "- **Label Ranking Average Precision**: {lrap:.4f}\n",
    "- **Precision@3**: {precision_at_3:.4f}\n",
    "\n",
    "### Confidence Scoring:\n",
    "- **Top-1 Accuracy**: {top_1_acc:.3f}\n",
    "- **Top-3 Accuracy**: {top_3_acc:.3f}\n",
    "- **Confidence Separation**: {conf_sep:.3f}\n",
    "\n",
    "### Key Achievements:\n",
    "1. ‚úì Trained and compared Random Forest and XGBoost models\n",
    "2. ‚úì Achieved strong performance on all evaluation metrics\n",
    "3. ‚úì Implemented comprehensive confidence scoring system\n",
    "4. ‚úì Validated confidence scores on test set\n",
    "5. ‚úì Performed thorough error analysis\n",
    "6. ‚úì Saved model for production deployment\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy model via FastAPI (Task 4)\n",
    "- Create comprehensive test suite\n",
    "- Generate API documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
